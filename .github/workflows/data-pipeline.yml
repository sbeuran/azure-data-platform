name: Data Pipeline

on:
  push:
    branches: [ main, develop ]
    paths: [ 'data/**' ]
  pull_request:
    branches: [ main ]
    paths: [ 'data/**' ]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC

env:
  AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
  AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
  AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
  AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
  DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

jobs:
  data-quality-check:
    name: Data Quality Check
    runs-on: ubuntu-latest
    environment: ${{ github.ref == 'refs/heads/main' && 'production' || 'development' }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install databricks-cli

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: |
          {
            "clientId": "${{ env.AZURE_CLIENT_ID }}",
            "clientSecret": "${{ env.AZURE_CLIENT_SECRET }}",
            "subscriptionId": "${{ env.AZURE_SUBSCRIPTION_ID }}",
            "tenantId": "${{ env.AZURE_TENANT_ID }}"
          }

    - name: Configure Databricks CLI
      run: |
        echo "[DEFAULT]" >> ~/.databrickscfg
        echo "host = ${{ env.DATABRICKS_HOST }}" >> ~/.databrickscfg
        echo "token = ${{ env.DATABRICKS_TOKEN }}" >> ~/.databrickscfg

    - name: Run Data Quality Tests
      run: |
        python scripts/data_quality_check.py

    - name: Upload Data Quality Report
      uses: actions/upload-artifact@v3
      with:
        name: data-quality-report
        path: reports/data_quality_report.html

  databricks-deploy:
    name: Deploy to Databricks
    runs-on: ubuntu-latest
    needs: data-quality-check
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
    environment: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install databricks-cli
        pip install databricks-connect

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: |
          {
            "clientId": "${{ env.AZURE_CLIENT_ID }}",
            "clientSecret": "${{ env.AZURE_CLIENT_SECRET }}",
            "subscriptionId": "${{ env.AZURE_SUBSCRIPTION_ID }}",
            "tenantId": "${{ env.AZURE_TENANT_ID }}"
          }

    - name: Configure Databricks CLI
      run: |
        echo "[DEFAULT]" >> ~/.databrickscfg
        echo "host = ${{ env.DATABRICKS_HOST }}" >> ~/.databrickscfg
        echo "token = ${{ env.DATABRICKS_TOKEN }}" >> ~/.databrickscfg

    - name: Deploy Notebooks
      run: |
        databricks workspace import_dir data/databricks/notebooks /SupplyChain/Notebooks --overwrite

    - name: Deploy Jobs
      run: |
        databricks jobs deploy --jobs-file data/databricks/jobs/supply_chain_etl.json
        databricks jobs deploy --jobs-file data/databricks/jobs/supply_chain_ml.json

    - name: Run ETL Pipeline
      run: |
        databricks jobs run-now --job-id ${{ secrets.ETL_JOB_ID }}

    - name: Run ML Pipeline
      run: |
        databricks jobs run-now --job-id ${{ secrets.ML_JOB_ID }}

  data-factory-deploy:
    name: Deploy Data Factory Pipelines
    runs-on: ubuntu-latest
    needs: databricks-deploy
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
    environment: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install azure-mgmt-datafactory
        pip install azure-identity

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: |
          {
            "clientId": "${{ env.AZURE_CLIENT_ID }}",
            "clientSecret": "${{ env.AZURE_CLIENT_SECRET }}",
            "subscriptionId": "${{ env.AZURE_SUBSCRIPTION_ID }}",
            "tenantId": "${{ env.AZURE_TENANT_ID }}"
          }

    - name: Deploy Data Factory Pipelines
      run: |
        python scripts/deploy_data_factory_pipelines.py

    - name: Trigger ETL Pipeline
      run: |
        python scripts/trigger_etl_pipeline.py

  synapse-deploy:
    name: Deploy Synapse Analytics
    runs-on: ubuntu-latest
    needs: data-factory-deploy
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
    environment: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install azure-mgmt-synapse
        pip install azure-identity

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: |
          {
            "clientId": "${{ env.AZURE_CLIENT_ID }}",
            "clientSecret": "${{ env.AZURE_CLIENT_SECRET }}",
            "subscriptionId": "${{ env.AZURE_SUBSCRIPTION_ID }}",
            "tenantId": "${{ env.AZURE_TENANT_ID }}"
          }

    - name: Deploy Synapse SQL Scripts
      run: |
        python scripts/deploy_synapse_sql.py

    - name: Create Synapse Views
      run: |
        python scripts/create_synapse_views.py

  monitoring-setup:
    name: Setup Monitoring
    runs-on: ubuntu-latest
    needs: synapse-deploy
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
    environment: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install azure-mgmt-monitor
        pip install azure-identity

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: |
          {
            "clientId": "${{ env.AZURE_CLIENT_ID }}",
            "clientSecret": "${{ env.AZURE_CLIENT_SECRET }}",
            "subscriptionId": "${{ env.AZURE_SUBSCRIPTION_ID }}",
            "tenantId": "${{ env.AZURE_TENANT_ID }}"
          }

    - name: Setup Monitoring Dashboards
      run: |
        python scripts/setup_monitoring_dashboards.py

    - name: Configure Alert Rules
      run: |
        python scripts/configure_alert_rules.py

  performance-test:
    name: Performance Test
    runs-on: ubuntu-latest
    needs: monitoring-setup
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
    environment: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install databricks-cli

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: |
          {
            "clientId": "${{ env.AZURE_CLIENT_ID }}",
            "clientSecret": "${{ env.AZURE_CLIENT_SECRET }}",
            "subscriptionId": "${{ env.AZURE_SUBSCRIPTION_ID }}",
            "tenantId": "${{ env.AZURE_TENANT_ID }}"
          }

    - name: Configure Databricks CLI
      run: |
        echo "[DEFAULT]" >> ~/.databrickscfg
        echo "host = ${{ env.DATABRICKS_HOST }}" >> ~/.databrickscfg
        echo "token = ${{ env.DATABRICKS_TOKEN }}" >> ~/.databrickscfg

    - name: Run Performance Tests
      run: |
        python scripts/performance_test.py

    - name: Upload Performance Report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: reports/performance_report.html

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: performance-test
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: 'data/'
        format: 'sarif'
        output: 'trivy-data-results.sarif'

    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-data-results.sarif'

    - name: Run Snyk security scan
      uses: snyk/actions/python@master
      env:
        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
      with:
        args: --severity-threshold=high

    - name: Upload Snyk scan results
      uses: actions/upload-artifact@v3
      with:
        name: snyk-results
        path: snyk.sarif
